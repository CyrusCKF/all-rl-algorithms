{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8180f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "class GridEnv(gym.Env):\n",
    "    def __init__(self, size=5):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "        # corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Each location is encoded as an element of {0, ..., `size`-1}^2\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=np.int64),\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=np.int64),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self._agent_location = np.array([-1, -1], dtype=int)\n",
    "        self._target_location = np.array([-1, -1], dtype=int)\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "\n",
    "    def reset(self, seed: int | None = None, options: dict | None = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action: int):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid bounds\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # An environment is completed if and only if the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        truncated = False\n",
    "        reward = (\n",
    "            1 if terminated else 0\n",
    "        )  # the agent is only reached at the end of the episode\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be7d4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7102f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Q-Network has as input a state s and outputs the state-action values q(s,a_1), ..., q(s,a_n) for all n actions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        x1 = F.leaky_relu(self.fc_1(inp))\n",
    "        x1 = F.leaky_relu(self.fc_2(x1))\n",
    "        x1 = self.fc_3(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1dfef314",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure for storing transitions\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# Define the Replay Memory class\n",
    "class ReplayMemory(object):\n",
    "    \"\"\" Stores transitions and allows sampling batches. \"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the Replay Memory.\n",
    "\n",
    "        Parameters:\n",
    "        - capacity (int): Maximum number of transitions to store.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Save a transition.\n",
    "\n",
    "        Parameters:\n",
    "        - *args: The transition elements (state, action, next_state, reward, done).\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int) -> list[Transition]:\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions from memory.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The number of transitions to sample.\n",
    "\n",
    "        Returns:\n",
    "        - List[Transition]: A list containing the sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Return the current size of the memory. \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdfec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def optimize_model(memory: ReplayMemory,\n",
    "                   policy_net: nn.Module,\n",
    "                   target_net: nn.Module,\n",
    "                   optimizer,\n",
    "                   batch_size: int,\n",
    "                   gamma: float,\n",
    "                   criterion: nn.Module = nn.SmoothL1Loss(), \n",
    "                   method:Literal[\"dqn\", \"ddqn\"]=\"ddqn\") -> float|None:\n",
    "    \"\"\"\n",
    "    Performs one step of optimization on the policy network.\n",
    "\n",
    "    Parameters:\n",
    "    - memory (ReplayMemory): The replay memory containing past transitions.\n",
    "    - policy_net (nn.Module): The main Q-network being optimized.\n",
    "    - target_net (nn.Module): The target Q-network used for stable target computation.\n",
    "    - optimizer (optim.Optimizer): The optimizer for updating the policy network.\n",
    "    - batch_size (int): The number of transitions to sample for each optimization step.\n",
    "    - gamma (float): The discount factor for future rewards.\n",
    "    - criterion (nn.Module): The loss function to use (default: SmoothL1Loss).\n",
    "\n",
    "    Returns:\n",
    "    - Optional[float]: The loss value for the optimization step, or None if not enough samples.\n",
    "    \"\"\"\n",
    "    # Ensure there are enough samples in memory to perform optimization\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # Sample a batch of transitions from replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))  # Unpack transitions into separate components\n",
    "\n",
    "    # Identify non-final states (states that are not terminal)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                  device=device, dtype=torch.bool)\n",
    "\n",
    "    # Stack non-final next states into a tensor\n",
    "\n",
    "    # Stack current states, actions, rewards, and dones into tensors\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    done_batch = torch.cat(batch.done)\n",
    "\n",
    "    # Compute Q(s_t, a) for the actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for the next states using the target network\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        if any(non_final_mask):  # Only compute for non-final states\n",
    "            non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "            if method == \"dqn\":\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "            elif method == \"ddqn\":\n",
    "                max_actions = policy_net(non_final_next_states).max(1)[1].unsqueeze(1)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, max_actions).squeeze(1)\n",
    "\n",
    "\n",
    "    # Compute the expected Q values using the Bellman equation\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute the loss between predicted and expected Q values\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Perform backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)  # Clip gradients to prevent explosion\n",
    "    optimizer.step()  # Update the policy network\n",
    "\n",
    "    return loss.item()  # Return the loss value for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def select_action(model, num_action, state, eps):\n",
    "    state_tensor = torch.Tensor(state[\"agent\"]).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = model(state_tensor)\n",
    "\n",
    "    # select a random action wih probability eps\n",
    "    if random.random() <= eps:\n",
    "        action = np.random.randint(0, num_action)\n",
    "    else:\n",
    "        action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(batch_size, current, target, optim, memory: ReplayMemory, gamma):\n",
    "\n",
    "    states, actions, next_states, rewards, is_done = memory.sample(batch_size)\n",
    "\n",
    "    q_values = current(states)\n",
    "\n",
    "    next_q_values = current(next_states)\n",
    "    next_q_state_values = target(next_states)\n",
    "\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "def evaluate(Qmodel, env, repeats, max_step):\n",
    "    \"\"\"\n",
    "    Runs a greedy policy with respect to the current Q-Network for \"repeats\" many episodes. Returns the average\n",
    "    episode reward.\n",
    "    \"\"\"\n",
    "    Qmodel.eval()\n",
    "    perform = 0\n",
    "    for _ in tqdm(range(repeats), \"Evaluate\"):\n",
    "        state, info = env.reset()\n",
    "        for _ in range(max_step):\n",
    "            agent_state = torch.Tensor(state[\"agent\"]).to(device)\n",
    "            with torch.no_grad():\n",
    "                values = Qmodel(agent_state)\n",
    "            action = np.argmax(values.cpu().numpy())\n",
    "            state, reward, term, trun, info = env.step(action)\n",
    "            perform += reward\n",
    "            \n",
    "            if term or trun:\n",
    "                break\n",
    "\n",
    "    Qmodel.train()\n",
    "    return perform/repeats\n",
    "\n",
    "\n",
    "def update_parameters(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8763f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized network components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 213.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  100\n",
      "rewards:  0.05\n",
      "lr:  0.001\n",
      "eps:  0.9841115531182099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 206.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  200\n",
      "rewards:  0.03\n",
      "lr:  0.001\n",
      "eps:  0.9646055206870082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 256.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  300\n",
      "rewards:  0.2\n",
      "lr:  0.001\n",
      "eps:  0.9454861164790007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 218.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  400\n",
      "rewards:  0.08\n",
      "lr:  0.001\n",
      "eps:  0.9267456771529368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 248.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  500\n",
      "rewards:  0.19\n",
      "lr:  0.001\n",
      "eps:  0.9083766912623201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 200.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  600\n",
      "rewards:  0.06\n",
      "lr:  0.001\n",
      "eps:  0.8903717962447101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 215.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  700\n",
      "rewards:  0.11\n",
      "lr:  0.001\n",
      "eps:  0.8727237754706968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 254.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  800\n",
      "rewards:  0.2\n",
      "lr:  0.001\n",
      "eps:  0.8554255553513692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 210.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  900\n",
      "rewards:  0.09\n",
      "lr:  0.001\n",
      "eps:  0.838470202503115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 207.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1000\n",
      "rewards:  0.07\n",
      "lr:  0.001\n",
      "eps:  0.8218509209686186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 220.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1100\n",
      "rewards:  0.1\n",
      "lr:  0.001\n",
      "eps:  0.805561049492939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 220.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1200\n",
      "rewards:  0.11\n",
      "lr:  0.001\n",
      "eps:  0.7895940588535815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 219.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1300\n",
      "rewards:  0.08\n",
      "lr:  0.001\n",
      "eps:  0.7739435492434863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 211.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1400\n",
      "rewards:  0.05\n",
      "lr:  0.001\n",
      "eps:  0.7586032477058929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 237.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1500\n",
      "rewards:  0.19\n",
      "lr:  0.001\n",
      "eps:  0.743567005620044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 249.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1600\n",
      "rewards:  0.23\n",
      "lr:  0.001\n",
      "eps:  0.7288287962367284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 249.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1700\n",
      "rewards:  0.19\n",
      "lr:  0.001\n",
      "eps:  0.7143827122626694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 231.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1800\n",
      "rewards:  0.1\n",
      "lr:  0.001\n",
      "eps:  0.7002229634927942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 231.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1900\n",
      "rewards:  0.12\n",
      "lr:  0.001\n",
      "eps:  0.6863438744894339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 222.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2000\n",
      "rewards:  0.09\n",
      "lr:  0.001\n",
      "eps:  0.6727398823075239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 229.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2100\n",
      "rewards:  0.11\n",
      "lr:  0.001\n",
      "eps:  0.6594055342648917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 214.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2200\n",
      "rewards:  0.05\n",
      "lr:  0.001\n",
      "eps:  0.6463354857567426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 206.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2300\n",
      "rewards:  0.07\n",
      "lr:  0.001\n",
      "eps:  0.6335244981134615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 230.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2400\n",
      "rewards:  0.13\n",
      "lr:  0.001\n",
      "eps:  0.6209674365008767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 229.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2500\n",
      "rewards:  0.18\n",
      "lr:  0.001\n",
      "eps:  0.6086592678621416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 225.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2600\n",
      "rewards:  0.1\n",
      "lr:  0.001\n",
      "eps:  0.5965950589004119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 221.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2700\n",
      "rewards:  0.09\n",
      "lr:  0.001\n",
      "eps:  0.5847699741015057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 221.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2800\n",
      "rewards:  0.12\n",
      "lr:  0.001\n",
      "eps:  0.5731792737957582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 100/100 [00:00<00:00, 206.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  2900\n",
      "rewards:  0.05\n",
      "lr:  0.001\n",
      "eps:  0.5618183122582913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(QNetwork(\n",
       "   (fc_1): Linear(in_features=2, out_features=64, bias=True)\n",
       "   (fc_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "   (fc_3): Linear(in_features=64, out_features=4, bias=True)\n",
       " ),\n",
       " [[100, 0.05],\n",
       "  [200, 0.03],\n",
       "  [300, 0.2],\n",
       "  [400, 0.08],\n",
       "  [500, 0.19],\n",
       "  [600, 0.06],\n",
       "  [700, 0.11],\n",
       "  [800, 0.2],\n",
       "  [900, 0.09],\n",
       "  [1000, 0.07],\n",
       "  [1100, 0.1],\n",
       "  [1200, 0.11],\n",
       "  [1300, 0.08],\n",
       "  [1400, 0.05],\n",
       "  [1500, 0.19],\n",
       "  [1600, 0.23],\n",
       "  [1700, 0.19],\n",
       "  [1800, 0.1],\n",
       "  [1900, 0.12],\n",
       "  [2000, 0.09],\n",
       "  [2100, 0.11],\n",
       "  [2200, 0.05],\n",
       "  [2300, 0.07],\n",
       "  [2400, 0.13],\n",
       "  [2500, 0.18],\n",
       "  [2600, 0.1],\n",
       "  [2700, 0.09],\n",
       "  [2800, 0.12],\n",
       "  [2900, 0.05]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    min_episodes=20,\n",
    "    eps=1,\n",
    "    eps_decay=0.998,\n",
    "    eps_min=0.01,\n",
    "    update_step=10,\n",
    "    batch_size=64,\n",
    "    update_repeats=50,\n",
    "    num_episodes=3000,\n",
    "    seed=42,\n",
    "    max_memory_size=5000,\n",
    "    lr_gamma=1,\n",
    "    lr_step=100,\n",
    "    measure_step=100,\n",
    "    measure_repeats=100,\n",
    "    hidden_dim=64,\n",
    "    cnn=False,\n",
    "    horizon=100,\n",
    "    render=True,\n",
    "    render_step=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Remark: Convergence is slow. Wait until around episode 2500 to see good performance.\n",
    "\n",
    "    :param gamma: reward discount factor\n",
    "    :param lr: learning rate for the Q-Network\n",
    "    :param min_episodes: we wait \"min_episodes\" many episodes in order to aggregate enough data before starting to train\n",
    "    :param eps: probability to take a random action during training\n",
    "    :param eps_decay: after every episode \"eps\" is multiplied by \"eps_decay\" to reduces exploration over time\n",
    "    :param eps_min: minimal value of \"eps\"\n",
    "    :param update_step: after \"update_step\" many episodes the Q-Network is trained \"update_repeats\" many times with a\n",
    "    batch of size \"batch_size\" from the memory.\n",
    "    :param batch_size: see above\n",
    "    :param update_repeats: see above\n",
    "    :param num_episodes: the number of episodes played in total\n",
    "    :param seed: random seed for reproducibility\n",
    "    :param max_memory_size: size of the replay memory\n",
    "    :param lr_gamma: learning rate decay for the Q-Network\n",
    "    :param lr_step: every \"lr_step\" episodes we decay the learning rate\n",
    "    :param measure_step: every \"measure_step\" episode the performance is measured\n",
    "    :param measure_repeats: the amount of episodes played in to asses performance\n",
    "    :param hidden_dim: hidden dimensions for the Q_network\n",
    "    :param env_name: name of the gym environment\n",
    "    :param cnn: set to \"True\" when using environments with image observations like \"Pong-v0\"\n",
    "    :param horizon: number of steps taken in the environment before terminating the episode (prevents very long episodes)\n",
    "    :param render: if \"True\" renders the environment every \"render_step\" episodes\n",
    "    :param render_step: see above\n",
    "    :return: the trained Q-Network and the measured performances\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env = GridEnv()\n",
    "\n",
    "    Q_1 = QNetwork(\n",
    "        action_dim=env.action_space.n,\n",
    "        state_dim=env.observation_space[\"agent\"].shape[0],\n",
    "        hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "    Q_2 = QNetwork(\n",
    "        action_dim=env.action_space.n,\n",
    "        state_dim=env.observation_space[\"agent\"].shape[0],\n",
    "        hidden_dim=hidden_dim,\n",
    "    ).to(device)\n",
    "    # transfer parameters from Q_1 to Q_2\n",
    "    update_parameters(Q_1, Q_2)\n",
    "\n",
    "    # we only train Q_1\n",
    "    for param in Q_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_1.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "    print(\"Initialized network components\")\n",
    "\n",
    "    memory = Memory(max_memory_size)\n",
    "    performance = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # display the performance\n",
    "        if (episode % measure_step == 0) and episode >= min_episodes:\n",
    "            performance.append([episode, evaluate(Q_1, env, measure_repeats, 100)])\n",
    "            # performance.append([episode, 0])\n",
    "            print(\"Episode: \", episode)\n",
    "            print(\"rewards: \", performance[-1][1])\n",
    "            print(\"lr: \", scheduler.get_last_lr()[0])\n",
    "            print(\"eps: \", eps)\n",
    "\n",
    "        state, info = env.reset()\n",
    "        memory.state.append(state)\n",
    "\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "            action = select_action(Q_2, env, state, eps)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if i > horizon:\n",
    "                done = True\n",
    "\n",
    "            # render the environment if render == True\n",
    "            if render and episode % render_step == 0:\n",
    "                try:\n",
    "                    env.render()\n",
    "                except NotImplementedError:\n",
    "                    pass\n",
    "\n",
    "            # save state, action, reward sequence\n",
    "            memory.update(state, action, reward, done)\n",
    "\n",
    "        has_trained = False\n",
    "        if episode >= min_episodes and episode % update_step == 0:\n",
    "            for _ in range(update_repeats):\n",
    "                train(batch_size, Q_1, Q_2, optimizer, memory, gamma)\n",
    "                has_trained = True\n",
    "\n",
    "            # transfer new parameter from Q_1 to Q_2\n",
    "            update_parameters(Q_1, Q_2)\n",
    "\n",
    "        # update learning rate and eps\n",
    "        if has_trained:\n",
    "            scheduler.step()\n",
    "            eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return Q_1, performance\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
