{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2aeea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for numerical computations, plotting, and utility functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up the device to use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set a random seed for reproducibility across runs\n",
    "seed = 42\n",
    "random.seed(seed)  # Seed for Python's random module\n",
    "np.random.seed(seed)  # Seed for NumPy\n",
    "torch.manual_seed(seed)  # Seed for PyTorch (CPU)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)  # Seed for PyTorch (GPU)\n",
    "\n",
    "# Enable inline plotting for Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0174c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Grid World Environment\n",
    "class GridEnvironment:\n",
    "    \"\"\"\n",
    "    A simple 10x10 Grid World environment.\n",
    "    State: (row, col) represented as normalized vector [row/10, col/10].\n",
    "    Actions: 0 (up), 1 (down), 2 (left), 3 (right).\n",
    "    Rewards: +10 for reaching the goal, -1 for hitting a wall, -0.1 for each step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows: int = 10, cols: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Grid World environment.\n",
    "\n",
    "        Parameters:\n",
    "        - rows (int): Number of rows in the grid.\n",
    "        - cols (int): Number of columns in the grid.\n",
    "        \"\"\"\n",
    "        self.rows: int = rows\n",
    "        self.cols: int = cols\n",
    "        self.start_state: Tuple[int, int] = (0, 0)  # Starting position\n",
    "        self.goal_state: Tuple[int, int] = (rows - 1, cols - 1)  # Goal position\n",
    "        self.state: Tuple[int, int] = self.start_state  # Current state\n",
    "        self.state_dim: int = 2  # State represented by 2 coordinates (row, col)\n",
    "        self.action_dim: int = 4  # 4 discrete actions: up, down, left, right\n",
    "\n",
    "        # Action mapping: maps action index to (row_delta, col_delta)\n",
    "        self.action_map: Dict[int, Tuple[int, int]] = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1)    # Right\n",
    "        }\n",
    "\n",
    "    def reset(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Resets the environment to the start state.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The initial state as a normalized tensor.\n",
    "        \"\"\"\n",
    "        self.state = self.start_state\n",
    "        return self._get_state_tensor(self.state)\n",
    "\n",
    "    def _get_state_tensor(self, state_tuple: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts a (row, col) tuple to a normalized tensor for the network.\n",
    "\n",
    "        Parameters:\n",
    "        - state_tuple (Tuple[int, int]): The state represented as a tuple (row, col).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized state as a tensor.\n",
    "        \"\"\"\n",
    "        # Normalize coordinates to be between 0 and 1\n",
    "        normalized_state: List[float] = [\n",
    "            state_tuple[0] / (self.rows - 1),\n",
    "            state_tuple[1] / (self.cols - 1)\n",
    "        ]\n",
    "        return torch.tensor(normalized_state, dtype=torch.float32, device=device)\n",
    "\n",
    "    def step(self, action: int) -> Tuple[torch.Tensor, float, bool]:\n",
    "        \"\"\"\n",
    "        Performs one step in the environment based on the given action.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take (0: up, 1: down, 2: left, 3: right).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, float, bool]: \n",
    "                - next_state_tensor (torch.Tensor): The next state as a normalized tensor.\n",
    "                - reward (float): The reward for the action.\n",
    "                - done (bool): Whether the episode has ended.\n",
    "        \"\"\"\n",
    "        # If the goal state is already reached, return the current state\n",
    "        if self.state == self.goal_state:\n",
    "            return self._get_state_tensor(self.state), 0.0, True\n",
    "\n",
    "        # Get the row and column deltas for the action\n",
    "        dr, dc = self.action_map[action]\n",
    "        current_row, current_col = self.state\n",
    "        next_row, next_col = current_row + dr, current_col + dc\n",
    "\n",
    "        # Default step cost\n",
    "        reward: float = -0.1\n",
    "        hit_wall: bool = False\n",
    "\n",
    "        # Check if the action leads to hitting a wall (out of bounds)\n",
    "        if not (0 <= next_row < self.rows and 0 <= next_col < self.cols):\n",
    "            # Stay in the same state and incur a penalty\n",
    "            next_row, next_col = current_row, current_col\n",
    "            reward = -1.0\n",
    "            hit_wall = True\n",
    "\n",
    "        # Update the state\n",
    "        self.state = (next_row, next_col)\n",
    "        next_state_tensor: torch.Tensor = self._get_state_tensor(self.state)\n",
    "\n",
    "        # Check if the goal state is reached\n",
    "        done: bool = (self.state == self.goal_state)\n",
    "        if done:\n",
    "            reward = 10.0  # Reward for reaching the goal\n",
    "\n",
    "        return next_state_tensor, reward, done\n",
    "\n",
    "    def get_action_space_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the size of the action space.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of possible actions (4).\n",
    "        \"\"\"\n",
    "        return self.action_dim\n",
    "\n",
    "    def get_state_dimension(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the dimension of the state representation.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of dimensions in the state (2).\n",
    "        \"\"\"\n",
    "        return self.state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0af2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-Network architecture\n",
    "class DQN(nn.Module):\n",
    "    \"\"\" Simple MLP Q-Network \"\"\"\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize the DQN.\n",
    "\n",
    "        Parameters:\n",
    "        - n_observations (int): Dimension of the state space.\n",
    "        - n_actions (int): Number of possible actions.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        # Define network layers\n",
    "        # Simple MLP: Input -> Hidden1 -> ReLU -> Hidden2 -> ReLU -> Output\n",
    "        self.layer1 = nn.Linear(n_observations, 128) # Input layer\n",
    "        self.layer2 = nn.Linear(128, 128)           # Hidden layer\n",
    "        self.layer3 = nn.Linear(128, n_actions)      # Output layer (Q-values for each action)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor representing the state(s).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor representing Q-values for each action.\n",
    "        \"\"\"\n",
    "        # Ensure input is float tensor\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "             x = torch.tensor(x, dtype=torch.float32, device=device)\n",
    "        elif x.dtype != torch.float32:\n",
    "             x = x.to(dtype=torch.float32)\n",
    "\n",
    "        # Apply layers with ReLU activation\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x) # Output layer has no activation (raw Q-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd64e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure for storing transitions\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "# Define the Replay Memory class\n",
    "class ReplayMemory(object):\n",
    "    \"\"\" Stores transitions and allows sampling batches. \"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the Replay Memory.\n",
    "\n",
    "        Parameters:\n",
    "        - capacity (int): Maximum number of transitions to store.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Save a transition.\n",
    "\n",
    "        Parameters:\n",
    "        - *args: The transition elements (state, action, next_state, reward, done).\n",
    "        \"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"\n",
    "        Sample a random batch of transitions from memory.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_size (int): The number of transitions to sample.\n",
    "\n",
    "        Returns:\n",
    "        - List[Transition]: A list containing the sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Return the current size of the memory. \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fd5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Selection (Epsilon-Greedy - Modified for single state tensor input)\n",
    "def select_action_custom(state: torch.Tensor,\n",
    "                         policy_net: nn.Module,\n",
    "                         epsilon_start: float,\n",
    "                         epsilon_end: float,\n",
    "                         epsilon_decay: int,\n",
    "                         n_actions: int) -> Tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Selects an action using the epsilon-greedy strategy for a single state tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - state (torch.Tensor): The current state as a tensor of shape [state_dim].\n",
    "    - policy_net (nn.Module): The Q-network used to estimate Q-values.\n",
    "    - epsilon_start (float): Initial value of epsilon (exploration rate).\n",
    "    - epsilon_end (float): Final value of epsilon after decay.\n",
    "    - epsilon_decay (int): Decay rate for epsilon (higher value means slower decay).\n",
    "    - n_actions (int): Number of possible actions.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[torch.Tensor, float]: \n",
    "        - The selected action as a tensor of shape [1, 1].\n",
    "        - The current epsilon value after decay.\n",
    "    \"\"\"\n",
    "    global steps_done_custom  # Counter to track the number of steps taken\n",
    "    sample = random.random()  # Generate a random number for epsilon-greedy decision\n",
    "    # Compute the current epsilon value based on the decay formula\n",
    "    epsilon_threshold = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "        math.exp(-1. * steps_done_custom / epsilon_decay)\n",
    "    steps_done_custom += 1  # Increment the step counter\n",
    "\n",
    "    if sample > epsilon_threshold:\n",
    "        # Exploitation: Choose the action with the highest Q-value\n",
    "        with torch.no_grad():\n",
    "            # Add a batch dimension to the state tensor to make it [1, state_dim]\n",
    "            state_batch = state.unsqueeze(0)\n",
    "            # Get the action with the maximum Q-value (output shape: [1, n_actions])\n",
    "            action = policy_net(state_batch).max(1)[1].view(1, 1)  # Reshape to [1, 1]\n",
    "    else:\n",
    "        # Exploration: Choose a random action\n",
    "        action = torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "    return action, epsilon_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513500aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(memory: ReplayMemory,\n",
    "                   policy_net: nn.Module,\n",
    "                   target_net: nn.Module,\n",
    "                   optimizer: optim.Optimizer,\n",
    "                   batch_size: int,\n",
    "                   gamma: float,\n",
    "                   criterion: nn.Module = nn.SmoothL1Loss()) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Performs one step of optimization on the policy network.\n",
    "\n",
    "    Parameters:\n",
    "    - memory (ReplayMemory): The replay memory containing past transitions.\n",
    "    - policy_net (nn.Module): The main Q-network being optimized.\n",
    "    - target_net (nn.Module): The target Q-network used for stable target computation.\n",
    "    - optimizer (optim.Optimizer): The optimizer for updating the policy network.\n",
    "    - batch_size (int): The number of transitions to sample for each optimization step.\n",
    "    - gamma (float): The discount factor for future rewards.\n",
    "    - criterion (nn.Module): The loss function to use (default: SmoothL1Loss).\n",
    "\n",
    "    Returns:\n",
    "    - Optional[float]: The loss value for the optimization step, or None if not enough samples.\n",
    "    \"\"\"\n",
    "    # Ensure there are enough samples in memory to perform optimization\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # Sample a batch of transitions from replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))  # Unpack transitions into separate components\n",
    "\n",
    "    # Identify non-final states (states that are not terminal)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                  device=device, dtype=torch.bool)\n",
    "\n",
    "    # Stack non-final next states into a tensor\n",
    "    if any(non_final_mask):  # Check if there are any non-final states\n",
    "        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Stack current states, actions, rewards, and dones into tensors\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    done_batch = torch.cat(batch.done)\n",
    "\n",
    "    # Compute Q(s_t, a) for the actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for the next states using the target network\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        if any(non_final_mask):  # Only compute for non-final states\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute the expected Q values using the Bellman equation\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute the loss between predicted and expected Q values\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Perform backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)  # Clip gradients to prevent explosion\n",
    "    optimizer.step()  # Update the policy network\n",
    "\n",
    "    return loss.item()  # Return the loss value for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99d6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_net(policy_net: nn.Module, target_net: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Copies the weights from the policy network to the target network.\n",
    "\n",
    "    Parameters:\n",
    "    - policy_net (nn.Module): The main Q-network whose weights are to be copied.\n",
    "    - target_net (nn.Module): The target Q-network to which weights are copied.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848db7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Custom Grid World\n",
    "BATCH_SIZE_CUSTOM = 128\n",
    "GAMMA_CUSTOM = 0.99         # Discount factor (encourage looking ahead)\n",
    "EPS_START_CUSTOM = 1.0      # Start with full exploration\n",
    "EPS_END_CUSTOM = 0.05       # End with 5% exploration\n",
    "EPS_DECAY_CUSTOM = 10000    # Slower decay for potentially larger state space exploration needs\n",
    "TAU_CUSTOM = 0.005          # Tau for soft updates (alternative, not used here)\n",
    "LR_CUSTOM = 5e-4            # Learning rate (might need tuning)\n",
    "MEMORY_CAPACITY_CUSTOM = 10000\n",
    "TARGET_UPDATE_FREQ_CUSTOM = 20 # Update target net less frequently maybe\n",
    "NUM_EPISODES_CUSTOM = 500      # More episodes might be needed\n",
    "MAX_STEPS_PER_EPISODE_CUSTOM = 200 # Max steps per episode (grid size related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff36f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate the custom GridEnvironment\n",
    "custom_env: GridEnvironment = GridEnvironment(rows=10, cols=10)\n",
    "\n",
    "# Get the size of the action space and state dimension\n",
    "n_actions_custom: int = custom_env.get_action_space_size()  # Number of possible actions (4)\n",
    "n_observations_custom: int = custom_env.get_state_dimension()  # Dimension of the state space (2)\n",
    "\n",
    "# Initialize the policy network (main Q-network) and target network\n",
    "policy_net_custom: DQN = DQN(n_observations_custom, n_actions_custom).to(device)  # Main Q-network\n",
    "target_net_custom: DQN = DQN(n_observations_custom, n_actions_custom).to(device)  # Target Q-network\n",
    "\n",
    "# Copy the weights from the policy network to the target network and set it to evaluation mode\n",
    "target_net_custom.load_state_dict(policy_net_custom.state_dict())  # Synchronize weights\n",
    "target_net_custom.eval()  # Set target network to evaluation mode\n",
    "\n",
    "# Initialize the optimizer for the policy network\n",
    "optimizer_custom: optim.AdamW = optim.AdamW(policy_net_custom.parameters(), lr=LR_CUSTOM, amsgrad=True)\n",
    "\n",
    "# Initialize the replay memory with the specified capacity\n",
    "memory_custom: ReplayMemory = ReplayMemory(MEMORY_CAPACITY_CUSTOM)\n",
    "\n",
    "# Lists for plotting\n",
    "episode_rewards_custom = []\n",
    "episode_lengths_custom = []\n",
    "episode_epsilons_custom = []\n",
    "episode_losses_custom = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891235ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN Training on Custom Grid World...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting DQN Training on Custom Grid World...\")\n",
    "\n",
    "# Initialize the global counter for epsilon decay\n",
    "steps_done_custom = 0\n",
    "\n",
    "# Training Loop\n",
    "for i_episode in range(NUM_EPISODES_CUSTOM):\n",
    "    # Reset the environment and get the initial state tensor\n",
    "    state = custom_env.reset()\n",
    "    total_reward = 0\n",
    "    current_losses = []\n",
    "\n",
    "    for t in range(MAX_STEPS_PER_EPISODE_CUSTOM):\n",
    "        # Select an action using epsilon-greedy policy\n",
    "        action_tensor, current_epsilon = select_action_custom(\n",
    "            state, policy_net_custom, EPS_START_CUSTOM, EPS_END_CUSTOM, EPS_DECAY_CUSTOM, n_actions_custom\n",
    "        )\n",
    "        action = action_tensor.item()\n",
    "\n",
    "        # Take a step in the environment\n",
    "        next_state_tensor, reward, done = custom_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Prepare tensors for storing in replay memory\n",
    "        reward_tensor = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        action_tensor_mem = torch.tensor([[action]], device=device, dtype=torch.long)\n",
    "        done_tensor = torch.tensor([done], device=device, dtype=torch.bool)\n",
    "\n",
    "        # Store the transition in replay memory\n",
    "        memory_next_state = next_state_tensor if not done else None\n",
    "        memory_custom.push(state, action_tensor_mem, memory_next_state, reward_tensor, done_tensor)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state_tensor\n",
    "\n",
    "        # Perform one optimization step on the policy network\n",
    "        loss = optimize_model(\n",
    "            memory_custom, policy_net_custom, target_net_custom, optimizer_custom, BATCH_SIZE_CUSTOM, GAMMA_CUSTOM\n",
    "        )\n",
    "        if loss is not None:\n",
    "            current_losses.append(loss)\n",
    "\n",
    "        # Break the loop if the episode is done\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store episode statistics\n",
    "    episode_rewards_custom.append(total_reward)\n",
    "    episode_lengths_custom.append(t + 1)\n",
    "    episode_epsilons_custom.append(current_epsilon)\n",
    "    episode_losses_custom.append(np.mean(current_losses) if current_losses else 0)\n",
    "\n",
    "    # Update the target network periodically\n",
    "    if i_episode % TARGET_UPDATE_FREQ_CUSTOM == 0:\n",
    "        update_target_net(policy_net_custom, target_net_custom)\n",
    "\n",
    "    # Print progress every 50 episodes\n",
    "    if (i_episode + 1) % 50 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_custom[-50:])\n",
    "        avg_length = np.mean(episode_lengths_custom[-50:])\n",
    "        avg_loss = np.mean([l for l in episode_losses_custom[-50:] if l > 0])\n",
    "        print(\n",
    "            f\"Episode {i_episode+1}/{NUM_EPISODES_CUSTOM} | \"\n",
    "            f\"Avg Reward (last 50): {avg_reward:.2f} | \"\n",
    "            f\"Avg Length: {avg_length:.2f} | \"\n",
    "            f\"Avg Loss: {avg_loss:.4f} | \"\n",
    "            f\"Epsilon: {current_epsilon:.3f}\"\n",
    "        )\n",
    "\n",
    "print(\"Custom Grid World Training Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ed5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results for Custom Grid World\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Rewards\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(episode_rewards_custom)\n",
    "plt.title('DQN Custom Grid: Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "rewards_ma_custom = np.convolve(episode_rewards_custom, np.ones(50)/50, mode='valid')\n",
    "if len(rewards_ma_custom) > 0: # Avoid plotting empty MA\n",
    "    plt.plot(np.arange(len(rewards_ma_custom)) + 49, rewards_ma_custom, label='50-episode MA', color='orange')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Lengths\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(episode_lengths_custom)\n",
    "plt.title('DQN Custom Grid: Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.grid(True)\n",
    "lengths_ma_custom = np.convolve(episode_lengths_custom, np.ones(50)/50, mode='valid')\n",
    "if len(lengths_ma_custom) > 0:\n",
    "    plt.plot(np.arange(len(lengths_ma_custom)) + 49, lengths_ma_custom, label='50-episode MA', color='orange')\n",
    "plt.legend()\n",
    "\n",
    "# Epsilon\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(episode_epsilons_custom)\n",
    "plt.title('DQN Custom Grid: Epsilon Decay')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(episode_losses_custom)\n",
    "plt.title('DQN Custom Grid: Episode Losses')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Losses')\n",
    "plt.grid(True)\n",
    "losses_ma_custom = np.convolve(episode_losses_custom, np.ones(50)/50, mode='valid')\n",
    "if len(losses_ma_custom) > 0:\n",
    "    plt.plot(np.arange(len(losses_ma_custom)) + 49, losses_ma_custom, label='50-episode MA', color='orange')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
